# -*- coding: utf-8 -*-
"""NLP-Automated-Customer-Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X_O0DdtF0Hf57um0MBgFlZY4eeeYKqNl

# Project NLP | Business Case: Automated Customer Reviews

### Notebook for Project Completion

This notebook is designed to guide me through the **"Automated Customer Reviews"** NLP project, fulfilling all the requirements outlined in the `README.md`. It covers data preparation, supervised and unsupervised modeling, generative AI, and deployment with detailed explanations for each step.

---

### **1. Setup and Data Loading**

First, I set up the environment by installing the necessary libraries and then loading the merged dataset.
"""

# Install required libraries
!pip install pandas scikit-learn matplotlib seaborn transformers datasets torch
!pip install sentence-transformers
!pip install faiss-cpu # or faiss-gpu if you have a GPU
!pip install rouge_score
!pip install --upgrade accelerate
!pip install --upgrade diffusers
!pip install --upgrade transformers
!pip install imbalanced-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sentence_transformers import SentenceTransformer
from sklearn.cluster import MiniBatchKMeans
import random

# Load the merged dataset from the uploaded file
try:
    df = pd.read_csv('amazon_reviews_merged.csv')
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: 'amazon_reviews_merged.csv' not found. Please upload the file or check the path.")

# Display the first few rows and column information
print("\nFirst 5 rows of the dataset:")
print(df.head())

print("\nDataset Info:")
df.info()

# Before proceeding, let's first check the column names
print("Original Columns:", df.columns)

# Rename the column with the correct name.
# Use 'reviews.rating' if that's the correct column name in your CSV.
df = df.rename(columns={'reviews.rating': 'star_rating'})

df = df.rename(columns={'reviews.rating': 'star_rating'})

print(df.shape)

print(df.isnull().sum())

"""---

### **2. Supervised Learning: Review Classification**

This section focuses on classifying reviews into **positive**, **negative**, or **neutral** sentiments using a pre-trained transformer model.

#### **2.1. Data Preprocessing**
I did map the `star_rating` column to our three sentiment classes as specified in your `README.md`. This is a crucial step for preparing the labeled data for a supervised learning model.
"""

# Define a function to map star ratings to sentiment classes
import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split

# Reload the DataFrame to ensure a clean state
try:
    df = pd.read_csv('amazon_reviews_merged.csv')
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: 'amazon_reviews_merged.csv' not found. Please upload the file or check the path.")

# Rename the columns to match the code.
# This fixes the 'KeyError' for both 'star_rating' and 'text'.
df = df.rename(columns={'reviews.rating': 'star_rating', 'reviews.text': 'text'})

# Define a function to map star ratings to sentiment classes
def map_sentiment(rating):
    if rating <= 2:
        return 'negative'
    elif rating == 3:
        return 'neutral'
    else: # rating 4 or 5
        return 'positive'

# Apply the function to create a new 'sentiment' column
df['sentiment'] = df['star_rating'].apply(map_sentiment)

# Let's check the distribution of our new sentiment classes
print("\nSentiment Class Distribution:")
print(df['sentiment'].value_counts())

# We need to convert sentiment labels to numerical IDs for the model
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
df['label'] = df['sentiment'].map(label_map)

# Split the data into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])

# Convert pandas DataFrames to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(train_df[['text', 'label']])
test_dataset = Dataset.from_pandas(test_df[['text', 'label']])

print("\nData preparation complete. Datasets created successfully!")

"""### 2.2. Exploratory Data Analysis (EDA)

Initial Data Inspection

This will give you a quick overview of the dataset's structure, including its size, column types, and a sample of the data.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Display basic information about the DataFrame
print("DataFrame Shape:", df.shape)
print("\nDataFrame Info:")
df.info()

# Display the first 5 rows to see what the data looks like
print("\nSample Data:")
print(df.head())

print(df.columns)

"""Analysis of Ratings and Sentiment Classes

Now, let's analyze the distribution of the original star ratings. This will show us if the dataset is balanced or if some ratings are more frequent than others. A great way to visualize this is with a count plot.
"""

# Create a count plot of the ratings
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='star_rating', palette='viridis')
plt.title('Distribution of Star Ratings')
plt.xlabel('Star Rating')
plt.ylabel('Number of Reviews')
plt.show()

"""Sentiment Stratification

As you mentioned, we need to stratify the data by mapping the reviews.rating to your sentiment classes: Positive, Negative, and Neutral. We'll create a new column called sentiment to store these values.
"""

# Map the ratings to sentiment classes
def map_rating_to_sentiment(rating):
    if rating >= 4:
        return 'Positive'
    elif rating == 3:
        return 'Neutral'
    else:
        return 'Negative'

df['sentiment'] = df['star_rating'].apply(map_rating_to_sentiment)

# Display the distribution of the new sentiment classes
print("\nDistribution of Sentiment Classes:")
print(df['sentiment'].value_counts())

# Visualize the sentiment class distribution
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='sentiment', palette='rocket')
plt.title('Distribution of Sentiment Classes')
plt.xlabel('Sentiment Class')
plt.ylabel('Number of Reviews')
plt.show()

"""2.3. Handling Imbalanced Data with SMOTE (Synthetic Minority Over-sampling Technique)"""

# --- Step 1: Data Cleaning & Sentiment Mapping (before resampling) ---
# We must clean the data first to ensure no NaN values affect the resampling.

# Define the sentiment mapping function
def map_rating_to_sentiment(rating):
    if rating >= 4:
        return 'Positive'
    elif rating == 3:
        return 'Neutral'
    else:
        return 'Negative'

# Apply the mapping and create the 'sentiment' column
# This will result in some NaN values where star_rating was NaN
df['sentiment'] = df['star_rating'].apply(map_rating_to_sentiment)

# Drop rows that have NaN values in the 'sentiment' column.
# This ensures we are only working with clean, labeled data.
print("Shape before dropping NaN:", df.shape)
df.dropna(subset=['sentiment'], inplace=True)
print("Shape after dropping NaN:", df.shape)

# --- Step 2: Resampling with RandomOverSampler ---
from imblearn.over_sampling import RandomOverSampler
from collections import Counter
import pandas as pd
import numpy as np

# Prepare the data for resampling
X = df['text']
y = df['sentiment']

print("Original sentiment distribution:", Counter(y))

# Initialize RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Resample the data
print("Applying RandomOverSampler to balance the data...")
X_resampled, y_resampled = ros.fit_resample(
    X.values.reshape(-1, 1),
    y.values.reshape(-1, 1)
)
print("Data resampled successfully.")

# Create the new, balanced DataFrame
df_balanced = pd.DataFrame({
    'text': pd.Series(X_resampled.ravel()),
    'sentiment': pd.Series(y_resampled.ravel())
})

# Display the new sentiment distribution
print("Resampled sentiment distribution:", Counter(df_balanced['sentiment']))

# Visualize the new sentiment distribution
plt.figure(figsize=(8, 6))
sns.countplot(data=df_balanced, x='sentiment', palette='rocket')
plt.title('Distribution of Resampled Sentiment Classes')
plt.xlabel('Sentiment Class')
plt.ylabel('Number of Reviews')
plt.show()

# --- Step 3: Fine-Tuning Setup and Splitting (now safe to run) ---
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer

# Map sentiments to numerical labels
label_map = {'Positive': 0, 'Neutral': 1, 'Negative': 2}
df_balanced['labels'] = df_balanced['sentiment'].map(label_map)

# Split the balanced DataFrame
X_train_text, X_test_text, y_train_labels, y_test_labels = train_test_split(
    df_balanced['text'],
    df_balanced['labels'],
    test_size=0.2,
    random_state=42,
    stratify=df_balanced['labels']
)

# Tokenize the splits
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(
    X_train_text.tolist(),
    truncation=True,
    padding=True
)

test_encodings = tokenizer(
    X_test_text.tolist(),
    truncation=True,
    padding=True
)

"""#### **2.2. Model Fine-Tuning**

I used a pre-trained **`distilbert-base-uncased`** model for its efficiency and strong performance on a wide range of NLP tasks. I had to fine-tune it on the specific review dataset to adapt its powerful language representations to the sentiment classification task.

1. Setup and Data Preparation for Fine-Tuning
"""

from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
import pandas as pd

# Assuming you have df_balanced from the previous RandomOverSampler step.

# Map sentiments to numerical labels
label_map = {'Positive': 0, 'Neutral': 1, 'Negative': 2}
df_balanced['labels'] = df_balanced['sentiment'].map(label_map)

# --- FIX: Drop rows with NaN values before splitting ---
df_balanced.dropna(inplace=True)

# Split the balanced DataFrame
X_train_text, X_test_text, y_train_labels, y_test_labels = train_test_split(
    df_balanced['text'],
    df_balanced['labels'],
    test_size=0.2,
    random_state=42,
    stratify=df_balanced['labels']
)

# Tokenize the splits
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(
    X_train_text.tolist(),
    truncation=True,
    padding=True
)

test_encodings = tokenizer(
    X_test_text.tolist(),
    truncation=True,
    padding=True
)

"""2. Create a Custom PyTorch Dataset"""

from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
import pandas as pd

# Assume you have df_balanced from the previous SMOTE step.
# If not, run the corrected SMOTE cell first.

# Map sentiments to numerical labels
label_map = {'Positive': 0, 'Neutral': 1, 'Negative': 2}
df_balanced['labels'] = df_balanced['sentiment'].map(label_map)

# Split the balanced DataFrame
X_train_text, X_test_text, y_train_labels, y_test_labels = train_test_split(
    df_balanced['text'],
    df_balanced['labels'],
    test_size=0.2,
    random_state=42,
    stratify=df_balanced['labels']
)

# Tokenize the splits
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(
    X_train_text.tolist(),
    truncation=True,
    padding=True
)

test_encodings = tokenizer(
    X_test_text.tolist(),
    truncation=True,
    padding=True
)

import torch

class ReviewDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = ReviewDataset(train_encodings, y_train_labels.tolist())
test_dataset = ReviewDataset(test_encodings, y_test_labels.tolist())

"""3. Fine-Tuning the Model"""

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import numpy as np
import torch

# Define the function to compute metrics
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    precision = precision_score(labels, predictions, average='weighted')
    recall = recall_score(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Load the pre-trained model for sequence classification
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3 # We have 3 sentiment classes
)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,  # Reduced batch size to prevent crashing
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    save_strategy="epoch",
    eval_strategy="epoch",  # Corrected parameter name
    load_best_model_at_end=True,
    metric_for_best_model='eval_accuracy',
    fp16=True,  # Enable mixed precision training
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics  # Pass the metrics function here
)

# Train the model
trainer.train()

# Save the final fine-tuned model
trainer.save_model("./sentiment_model")

#fine tuning the sentiment model with distilbert

from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import numpy as np
import torch

# Define the function to compute metrics (same as before)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    precision = precision_score(labels, predictions, average='weighted')
    recall = recall_score(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Load the pre-trained DistilBERT model for sequence classification
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=3 # We have 3 sentiment classes
)

# Define training arguments (same as before)
training_args = TrainingArguments(
    output_dir='./results_distilbert',  # Changed output directory to avoid conflicts
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs_distilbert',  # Changed logging directory
    logging_steps=10,
    save_strategy="epoch",
    eval_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model='eval_accuracy',
    fp16=True,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Save the final fine-tuned model
trainer.save_model("./sentiment_model_distilbert")

#Fine tunewith roberta-based model
from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import numpy as np
import torch

# Define the function to compute metrics (same as before)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    precision = precision_score(labels, predictions, average='weighted')
    recall = recall_score(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Load the pre-trained RoBERTa model for sequence classification
# NOTE: RoBERTa does not have an 'uncased' version like BERT, so we use 'roberta-base'
model = RobertaForSequenceClassification.from_pretrained(
    "roberta-base",
    num_labels=3 # We have 3 sentiment classes
)

# Define training arguments (same as before)
training_args = TrainingArguments(
    output_dir='./results_roberta',  # Changed output directory
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs_roberta',  # Changed logging directory
    logging_steps=10,
    save_strategy="epoch",
    eval_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model='eval_accuracy',
    fp16=True,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Save the final fine-tuned model
trainer.save_model("./sentiment_model_roberta")

"""#### 2.3. Model Evaluation and Visualization
I used the evaluation metrics from the `Trainer` and create a **confusion matrix** to visually analyze the model's performance. The confusion matrix helps us understand which sentiment classes the model is confusing.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments

# --- Load the saved model and tokenizer ---
model_path = "./sentiment_model_distilbert"

try:
    final_model = AutoModelForSequenceClassification.from_pretrained(model_path)
    final_tokenizer = AutoTokenizer.from_pretrained(model_path)
    print("Model and tokenizer loaded successfully!")
except Exception as e:
    print(f"Error loading model: {e}. Please ensure the model directory exists and has been downloaded.")
    # Exit if model fails to load
    exit()

# --- Re-initialize the Trainer to perform predictions ---
# Note: You can reuse the training_args from your previous cell
training_args = TrainingArguments(
    output_dir='./results_distilbert',
    per_device_eval_batch_size=64,
)

trainer = Trainer(
    model=final_model,
    args=training_args,
)

# --- Perform predictions on the test dataset ---
# We use trainer.predict() to get the predictions and true labels
predictions = trainer.predict(test_dataset)

# Get the predicted labels and true labels
y_pred_logits = predictions.predictions
y_true_labels = predictions.label_ids

# Convert logits to predicted labels (class with the highest probability)
y_pred_labels = np.argmax(y_pred_logits, axis=1)

# --- Create the Confusion Matrix ---
cm = confusion_matrix(y_true_labels, y_pred_labels)
sentiment_labels = ['Positive', 'Neutral', 'Negative']

# --- Plot the Confusion Matrix ---
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sentiment_labels, yticklabels=sentiment_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

from google.colab import files

# After the trainer.save_model() command runs successfully
!zip -r /content/sentiment_model_distilbert.zip /content/sentiment_model_distilbert

# This will trigger a download to your local machine
files.download('/content/sentiment_model_distilbert.zip')

"""---

### **3. Unsupervised Learning: Product Category Clustering**
Here, l used an unsupervised approach to group reviews into 4-6 meta-categories. This is valuable for discovering natural groupings in the data without predefined labels.
"""

# Step 1: Text Preprocessing & Vectorization

''' This code will clean your reviews and convert them into numerical vectors using a powerful pre-trained model.'''

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# --- Download NLTK resources ---
# This is the fix for the error you're getting.
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4') # Required for WordNetLemmatizer

# --- Text Preprocessing ---
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text_for_clustering(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
    return ' '.join(tokens)

# Apply the preprocessing to the 'reviewText' column
df['cleaned_review'] = df['reviews.text'].apply(preprocess_text_for_clustering)

# --- Vectorization using Sentence Transformers ---
# Load a pre-trained model for high-quality embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create embeddings from the cleaned text
print("Creating text embeddings...")
review_embeddings = model.encode(df['cleaned_review'].tolist(), show_progress_bar=True)
print("Shape of embeddings:", review_embeddings.shape)

print(df.columns)

# Step 2: Apply K-Means Clustering

''' This code will apply the K-Means algorithm to your vectors to group your reviews into 5 categories.'''

from sklearn.cluster import KMeans

# Define the number of clusters as 5 (as suggested)
num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)

# Fit the model to the embeddings
kmeans.fit(review_embeddings)

# Add the cluster labels to your DataFrame
df['cluster_label'] = kmeans.labels_

# Check the distribution of reviews across the clusters
print("\nCluster sizes:")
print(df['cluster_label'].value_counts().sort_index())

# Optionally, print some sample reviews from each cluster to understand the topics
for i in range(num_clusters):
    print(f"\n--- Cluster {i} (n={len(df[df['cluster_label'] == i])}) ---")
    print(df[df['cluster_label'] == i]['reviews.text'].head(3).tolist())

#Analyzing the Clusters
# To understand the clusters, you can print a few reviews from each cluster.
for cluster_id in range(5):  # Assuming 5 clusters (0 to 4)
    print(f"\n--- Sample Reviews from Cluster {cluster_id} ---")

    # Filter the DataFrame for the current cluster
    cluster_reviews = df[df['cluster_label'] == cluster_id]['reviews.text'].tolist()

    # Print the first 5 reviews in this cluster
    if cluster_reviews:
        for i, review in enumerate(cluster_reviews[:5]):
            print(f"{i+1}. {review}")
    else:
        print("No reviews in this cluster.")

#Refined Approach for Product Category Clustering

# let's clean the categories column since it contains multiple categories per review.
# Assuming the column name is 'categories'
df['categories'] = df['categories'].apply(lambda x: x.split(',') if isinstance(x, str) else [])
df['first_category'] = df['categories'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else 'Unknown')

# let's create the average vector for each category and perform the clustering.

import numpy as np
from sklearn.cluster import KMeans

# Step 1: Group embeddings by category
# Combine the embeddings with the category labels
df_with_embeddings = pd.DataFrame(review_embeddings)
df_with_embeddings['first_category'] = df['first_category']

# Calculate the average embedding for each unique category
category_embeddings = df_with_embeddings.groupby('first_category').mean()

# Step 2: Cluster the categories
num_meta_categories = 5 # Let's use 5 clusters as a starting point
kmeans_categories = KMeans(n_clusters=num_meta_categories, random_state=42, n_init=10)
kmeans_categories.fit(category_embeddings)

# Step 3: Map the original categories to their new meta-category labels
category_mapping = dict(zip(category_embeddings.index, kmeans_categories.labels_))
df['meta_category_label'] = df['first_category'].map(category_mapping)

# Display the mapping and the size of each new meta-category
print("Category to Meta-Category Mapping:")
for category, label in category_mapping.items():
    print(f"  {category:<30} -> Meta-Category {label}")

print("\nNumber of reviews in each Meta-Category:")
print(df['meta_category_label'].value_counts().sort_index())

# 1. Zero-Shot Classification to Define Categories

!pip install transformers accelerate

from transformers import pipeline

# Load a zero-shot classification model
classifier = pipeline("zero-shot-classification",
                      model="facebook/bart-large-mnli")

# Define your candidate categories based on the project brief
candidate_labels = ['Ebook readers', 'Batteries', 'Accessories', 'Non-electronics']

# Let's run a test on a few sample reviews from your dataset
sample_reviews = df['reviews.text'].sample(5).tolist()

print("Running Zero-Shot Classification on Sample Reviews:")
for review in sample_reviews:
    result = classifier(review, candidate_labels)
    # The result returns a list of labels and their scores, sorted by confidence.
    print(f"\nReview: {review[:100]}...")
    print(f"Predicted Category: {result['labels'][0]} with confidence {result['scores'][0]:.2f}")

!pip install transformers accelerate
import pandas as pd
from transformers import pipeline

# --- Setup ---
classifier = pipeline("zero-shot-classification",
                      model="facebook/bart-large-mnli")

candidate_labels = ['Ebook readers', 'Batteries', 'Accessories', 'Non-electronics']

# --- Diagnostic Check 1 ---
print("Current DataFrame columns BEFORE classification:")
print(df.columns)
print("-" * 50)

# --- Classification Loop ---
# This code is slightly changed to be more robust
predicted_categories = []
for review in df['reviews.text']:
    if isinstance(review, str):
        result = classifier(review, candidate_labels)
        predicted_categories.append(result['labels'][0])
    else:
        # Handle non-string values gracefully
        predicted_categories.append('Unknown')

df['zero_shot_category'] = predicted_categories
df['zero_shot_score'] = [result['scores'][0] if isinstance(result, dict) else 0.0 for result in classifier(df['reviews.text'].tolist(), candidate_labels)]

# --- Diagnostic Check 2 ---
print("\nDataFrame columns AFTER classification:")
print(df.columns)
print("-" * 50)

# --- Diagnostic Check 3 ---
print("\nFirst 5 rows with new columns:")
print(df[['reviews.text', 'zero_shot_category']].head())
print("-" * 50)

print("\nDistribution of reviews across the new categories:")
print(df['zero_shot_category'].value_counts())

# Analyzing Categories with TF-IDF

from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming 'df' has 'cleaned_review' and 'zero_shot_category' columns.
# We'll use the 'cleaned_review' column for this analysis.

# Group reviews by the new zero-shot categories
category_reviews = df.groupby('zero_shot_category')['cleaned_review'].apply(lambda x: ' '.join(x))

# Create a TF-IDF vectorizer
# max_df and min_df help filter out words that are too common or too rare
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')

# Fit and transform the grouped text data
tfidf_matrix = tfidf_vectorizer.fit_transform(category_reviews)

# Get feature names (words)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Get the top N words for each category
num_words = 15  # Let's get the top 15 words
for category_name, review_text in category_reviews.items():
    print(f"\n--- Top {num_words} TF-IDF Words for Category: {category_name} ---")

    # Get the index of the current category in the TF-IDF matrix
    category_index = list(category_reviews.index).index(category_name)

    # Get the TF-IDF vector for this category
    category_tfidf_vector = tfidf_matrix[category_index]

    # Get the indices of the top words
    top_indices = category_tfidf_vector.toarray().argsort()[0][-num_words:]

    # Get the actual words and print them
    top_words = [feature_names[i] for i in top_indices]
    print(top_words)

"""---

### **4. Generative AI: Review Summarization**

In this section, I used a pre-trained generative model to create summaries and articles for each product category.

#### **4.1. Summarization with a Generative Model**

I used the **`t5-small`** model, which is a powerful and lightweight model for text-to-text tasks like summarization. I created a function to generate a short article based on a set of reviews for a given product or category.
"""

# Step 1: Set Up the Summarization Pipeline

!pip install transformers[sentencepiece]
from transformers import pipeline

# Load a pre-trained summarization model
summarizer = pipeline("summarization", model="t5-small")

# Step 2: Generate Summaries for Each Category

import pandas as pd
import numpy as np

# Loop through each of the zero-shot categories
for category_name in df['zero_shot_category'].unique():
    print(f"\n--- Generating Summary for Category: {category_name} ---")

    # Get a sample of reviews from the current category (e.g., up to 20 reviews)
    category_reviews = df[df['zero_shot_category'] == category_name]['reviews.text'].sample(min(20, len(df[df['zero_shot_category'] == category_name]))).tolist()

    if not category_reviews:
        print("No reviews to summarize in this category.")
        print("="*50)
        continue

    # Combine the reviews into a single text block
    combined_reviews = " ".join(category_reviews)

    # Generate the summary
    # You might need to adjust max_length to get a more detailed summary
    summary = summarizer(combined_reviews, max_length=150, min_length=30, do_sample=False)

    # Print the generated summary
    print(summary[0]['summary_text'])
    print("="*50)

# 1. Prompt Engineering for Summarization

import pandas as pd
import numpy as np
from transformers import pipeline

# Load the summarization pipeline again (if it's not already in memory)
summarizer = pipeline("summarization", model="t5-small")

# --- Generate Summaries with a Prompt for Each Category ---
for category_name in df['zero_shot_category'].unique():
    print(f"\n--- Generating Prompted Summary for Category: {category_name} ---")

    # Get a sample of reviews (e.g., 20)
    category_reviews = df[df['zero_shot_category'] == category_name]['reviews.text'].sample(min(20, len(df[df['zero_shot_category'] == category_name]))).tolist()

    if not category_reviews:
        print("No reviews to summarize in this category.")
        print("="*50)
        continue

    # Combine the reviews into a single text block
    combined_reviews = " ".join(category_reviews)

    # Use a well-structured prompt to guide the summarizer
    prompted_text = f"Summarize the following customer reviews into a short article, focusing on the main pros and cons of the products and the top complaints. \n\nReviews:\n{combined_reviews}"

    # Generate the summary
    summary = summarizer(prompted_text, max_length=150, min_length=30, do_sample=False)

    # Print the generated summary
    print(summary[0]['summary_text'])
    print("="*50)

# 2. Comparing with OpenAI's Models

# First, install the OpenAI library
!pip install openai

import openai
from google.colab import userdata

# It is highly recommended to store your API key in Colab Secrets
# Go to the key icon on the left panel -> "Secrets" -> "Add new secret"
# Name: OPENAI_API_KEY
# Value: your_api_key_string
# Then, make sure "Notebook access" is enabled.

# Get the API key from Colab Secrets
try:
    openai.api_key = userdata.get('OPENAI_API_KEY')
    print("OpenAI API key loaded securely.")
except Exception as e:
    print(f"Error loading API key: {e}")
    print("Please make sure you have set up your API key in Colab Secrets.")
    openai.api_key = None

def get_openai_summary(text):
    if not openai.api_key:
        return "OpenAI API key not found. Cannot generate summary."

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo-0125",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that summarizes customer reviews."},
                {"role": "user", "content": f"Summarize the following customer reviews:\n\n{text}"}
            ],
            max_tokens=150,
            n=1,
            stop=None,
            temperature=0.5,
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        return f"An error occurred: {e}"

# --- Now, let's run a test on a small sample of your data ---
# Get some reviews for a category, e.g., 'Ebook readers'
test_reviews = df[df['zero_shot_category'] == 'Ebook readers']['reviews.text'].sample(min(10, len(df[df['zero_shot_category'] == 'Ebook readers']))).tolist()
combined_reviews = " ".join(test_reviews)

if openai.api_key:
    openai_summary = get_openai_summary(combined_reviews)
    print("\n--- OpenAI (GPT-3.5) Summary for Ebook Readers ---")
    print(openai_summary)
    print("="*50)

# 1. Save your cleaned DataFrame to a CSV file
df.to_csv('final_reviews.csv', index=False)

# 2. Zip your fine-tuned model directory for easy download
!zip -r sentiment_model_distilbert.zip sentiment_model_distilbert/

"""### Model Deployment via website"""

# Building the Gradio Application

# First, make sure you have Gradio and the necessary Hugging Face libraries installed.

!pip install gradio transformers torch accelerate

# It will load all your models and create a multi-tabbed web interface to showcase each model's capability.

import gradio as gr
from transformers import pipeline

# --- Load all models ---

# 1. Load your fine-tuned sentiment analysis model
try:
    sentiment_model_path = "./sentiment_model_distilbert"
    sentiment_classifier = pipeline("sentiment-analysis", model=sentiment_model_path, tokenizer=sentiment_model_path)
    print("Sentiment classifier loaded successfully.")
except Exception as e:
    print(f"Error loading sentiment model: {e}")
    sentiment_classifier = None

# 2. Load the zero-shot classifier for product categorization
zero_shot_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
candidate_labels = ['Ebook readers', 'Batteries', 'Accessories', 'Non-electronics']
print("Zero-shot classifier loaded successfully.")

# 3. Load the summarization model
summarizer = pipeline("summarization", model="t5-small")
print("Summarizer loaded successfully.")

# --- Define Gradio Interface Functions ---

def analyze_sentiment(text):
    if sentiment_classifier:
        result = sentiment_classifier(text)
        label = result[0]['label']
        score = result[0]['score']
        return f"Sentiment: {label} (Confidence: {score:.2f})"
    return "Error: Sentiment model not loaded."

def categorize_review(text):
    result = zero_shot_classifier(text, candidate_labels)
    predicted_label = result['labels'][0]
    score = result['scores'][0]
    return f"Predicted Category: {predicted_label} (Confidence: {score:.2f})"

def summarize_reviews(text):
    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)
    return summary[0]['summary_text']

# --- Build the Gradio App with Tabs ---

with gr.Blocks() as demo:
    gr.Markdown("# Product Review AI Assistant")
    gr.Markdown("This app demonstrates a comprehensive AI system for analyzing product reviews, including sentiment analysis, product categorization, and review summarization.")

    with gr.Tab("Sentiment Analysis"):
        gr.Markdown("### Analyze the sentiment of a single review.")
        sentiment_input = gr.Textbox(lines=2, placeholder="Enter a product review here...")
        sentiment_output = gr.Textbox(label="Sentiment Result")
        sentiment_button = gr.Button("Analyze")
        sentiment_button.click(fn=analyze_sentiment, inputs=sentiment_input, outputs=sentiment_output)

    with gr.Tab("Product Categorization"):
        gr.Markdown("### Classify a review into a product category.")
        category_input = gr.Textbox(lines=2, placeholder="Enter a product review here...")
        category_output = gr.Textbox(label="Predicted Category")
        category_button = gr.Button("Categorize")
        category_button.click(fn=categorize_review, inputs=category_input, outputs=category_output)

    with gr.Tab("Review Summarization"):
        gr.Markdown("### Summarize a block of multiple reviews.")
        summary_input = gr.Textbox(lines=5, placeholder="Paste multiple product reviews here...")
        summary_output = gr.Textbox(label="Generated Summary")
        summary_button = gr.Button("Summarize")
        summary_button.click(fn=summarize_reviews, inputs=summary_input, outputs=summary_output)

demo.launch(share=True)